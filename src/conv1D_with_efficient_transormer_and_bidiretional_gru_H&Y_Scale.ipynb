{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b5c119-3826-4047-9a6e-cb3bc6fd95ad",
      "metadata": {
        "id": "17b5c119-3826-4047-9a6e-cb3bc6fd95ad",
        "outputId": "71dffe53-50a7-4ad6-adc0-686d842c00bb",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PATH: /usr/local/cuda-11.8/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
            "CUDA_HOME: /usr/local/cuda-11.8\n"
          ]
        }
      ],
      "source": [
        "# Add ptxas to Your PATH\n",
        "# replace the path by the outcome of \"find / -name ptxas 2>/dev/null\"\n",
        "\n",
        "import os\n",
        "# Update PATH to include ptxas location\n",
        "os.environ['PATH'] = '/usr/local/cuda-11.8/bin:' + os.environ['PATH']\n",
        "\n",
        "# Set CUDA_HOME environment variable\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-11.8'\n",
        "\n",
        "# Verify the updated PATH and CUDA_HOME\n",
        "print(\"PATH:\", os.environ['PATH'])\n",
        "print(\"CUDA_HOME:\", os.environ['CUDA_HOME'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EudLrOGCpliR",
      "metadata": {
        "id": "EudLrOGCpliR",
        "outputId": "0f2e51ba-27cb-48e9-dfee-dc982f9075e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-02 14:43:22.941517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "#disable tensor float 32 for accuracy\n",
        "import tensorflow as tf\n",
        "tf.config.experimental.enable_tensor_float_32_execution(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5122fb48-b2bc-41aa-b260-7a59ce9b81cc",
      "metadata": {
        "id": "5122fb48-b2bc-41aa-b260-7a59ce9b81cc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "np.random.seed(2)\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import GRU, Dropout, BatchNormalization, Conv1D, Dense, GlobalAveragePooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.layers import Input, MaxPooling1D, UpSampling1D, LayerNormalization, Bidirectional\n",
        "\n",
        "def efficient_transformer_block(x, num_heads, proj_dim):\n",
        "    #Efficient Transformer Encoder Block with Layer Normalization, Multi-Head Attention, and Feed-Forward Network\n",
        "    x_norm = layers.LayerNormalization()(x)\n",
        "    attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=proj_dim, dropout=0.1)(x_norm, x_norm)\n",
        "    x_skip = layers.Add()([x, attention_output])  # Skip Connection\n",
        "\n",
        "    x_norm = layers.LayerNormalization()(x_skip)\n",
        "    #ff_output = layers.Dense(proj_dim * 2, activation='relu')(x_norm)\n",
        "    ff_output = layers.Dense(proj_dim * 2, activation='relu', kernel_regularizer=l2(0.001))(x_norm)\n",
        "    ff_output = layers.Dense(x.shape[-1], activation='relu')(ff_output)\n",
        "    return layers.Add()([x_skip, ff_output])  # Second Skip Connection\n",
        "\n",
        "def conv1D_with_efficient_transformer_and_bidirec_gru():\n",
        "    input_ = Input(shape=(100, 1))\n",
        "\n",
        "    # Conv1D Branch with padding='same'\n",
        "    conv_path = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_)\n",
        "    conv_path = BatchNormalization()(conv_path)\n",
        "    conv_path = Dropout(0.4)(conv_path)\n",
        "\n",
        "    # Transformer Branch\n",
        "    trans_path = efficient_transformer_block(input_, num_heads=2, proj_dim=64)\n",
        "    trans_path = BatchNormalization()(trans_path)\n",
        "    trans_path = Dropout(0.4)(trans_path)\n",
        "\n",
        "    # Concatenate Conv1D and Transformer\n",
        "    x = layers.concatenate([conv_path, trans_path], axis=-1)\n",
        "\n",
        "    # GRU Layer for Sequential Processing\n",
        "    x = Bidirectional(GRU(units=64, return_sequences=True))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    # Pooling and Dense Layers\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(50, activation='elu')(x)\n",
        "\n",
        "    return input_, x\n",
        "\n",
        "\n",
        "def multiple_cnn1D5_level(nb):\n",
        "    '''\n",
        "    Model for severity prediction, 5 classes output\n",
        "    :param nb:  number of parallel branches\n",
        "    :return:\n",
        "    '''\n",
        "    # Initialize with the first input and branch\n",
        "    input_, CNN_ = conv1D_with_efficient_transformer_and_bidirec_gru()\n",
        "    inputs = [input_]\n",
        "    CNNs = [CNN_]\n",
        "\n",
        "    for i in range(1, nb):\n",
        "        input_i, CNN_i = conv1D_with_efficient_transformer_and_bidirec_gru()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    x = layers.concatenate(CNNs, axis=-1)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = layers.Dense(100, activation='selu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = layers.Dense(20, activation='selu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    answer = layers.Dense(4, activation='softmax')(x)\n",
        "    model = Model(inputs, answer)\n",
        "    opt = optimizers.Nadam(learning_rate=0.001)  # Updated to use 'learning_rate' instead of 'lr'\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    #print(model.summary())\n",
        "    return model\n",
        "\n",
        "def multiple_cnn1D(nb):\n",
        "    '''\n",
        "    :param nb: number of features ( indicates the number of parallel branches)\n",
        "    :return:\n",
        "    '''\n",
        "    # initialise with the first input\n",
        "    input_, CNN_ = conv1D_with_efficient_transformer_and_bidirec_gru()\n",
        "    inputs = [input_]\n",
        "    CNNs = [CNN_]\n",
        "\n",
        "    for i in range(1, nb):\n",
        "        input_i, CNN_i = conv1D_with_efficient_transformer_and_bidirec_gru()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    x = layers.concatenate(CNNs, axis=-1)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = layers.Dense(100, activation='selu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = layers.Dense(20, activation='selu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    answer = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs, answer)\n",
        "    opt = optimizers.RMSprop(learning_rate=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    #print(model.summary())\n",
        "    return model\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#sys.path.append('../src')\n",
        "np.random.seed(2)\n",
        "\n",
        "\n",
        "\n",
        "class Data:\n",
        "\n",
        "    def __init__(self,  input_data,  deep, gait_cycle, step=50, features=np.arange(1, 19), pk_level = True):\n",
        "        '''\n",
        "\n",
        "        :param load_or_get:  1: load data , 0: load preloaded datas ( npy)\n",
        "        :param deep:  data in the format for deep learning algorithms\n",
        "        :param gait_cycle: number of gait cycle per signal\n",
        "        :param step: overlap between gait signals\n",
        "        :param features: signals to be loaded ( coming from sensors)\n",
        "        :param pk_level: if true , y is the parkinson level according\n",
        "        '''\n",
        "\n",
        "        self.deep = deep\n",
        "        self.step = step\n",
        "        self.nb_gait_cycle = gait_cycle\n",
        "\n",
        "\n",
        "        self.features_to_load = features\n",
        "        self.nb_features = self.features_to_load.shape[0]\n",
        "        ###############\n",
        "        self.X_data = np.array([])  # np.ones((self.nb_gait_cycle,self.nb_features))\n",
        "        self.y_data = np.array([])\n",
        "        self.nb_data_per_person = np.array([0])\n",
        "\n",
        "\n",
        "        files = sorted(glob.glob(os.path.join(input_data, '*txt')))\n",
        "        self.ctrl_list = []\n",
        "        self.pk_list = []\n",
        "        for file in files:\n",
        "\n",
        "            if file.find(\".txt\") != -1:\n",
        "                if file.find(\"Co\") != -1:  # if control\n",
        "                    self.ctrl_list.append(file)\n",
        "                elif file.find(\"Pt\") != -1:  # if parkinsonian\n",
        "                    self.pk_list.append(file)\n",
        "\n",
        "        random.shuffle(self.ctrl_list)\n",
        "        random.shuffle(self.pk_list)\n",
        "        self.pk_level = pk_level\n",
        "        if pk_level == True:\n",
        "            self.levels = pd.read_csv( os.path.join(input_data, \"demographics.csv\"))\n",
        "            self.levels.set_index('ID', inplace=True)\n",
        "        self.load(norm=None)\n",
        "\n",
        "\n",
        "    def separate_fold(self, fold_number, total_fold=10):\n",
        "        '''\n",
        "\n",
        "        :param fold_number: Fold number\n",
        "        :param total_fold: Total number of fols\n",
        "        :return:\n",
        "        '''\n",
        "        proportion = 1 / total_fold  # .10 for 10 folds\n",
        "        X = [self.X_ctrl, self.X_park]\n",
        "        y = [self.y_ctrl, self.y_park]\n",
        "        patients = [self.nb_data_per_person[:self.last_ctrl_patient], self.nb_data_per_person[self.last_ctrl_patient:]] # counts separated by classe\n",
        "        patients[1]= patients[1] - patients[1][0]\n",
        "        diff_count = np.diff(self.nb_data_per_person)\n",
        "        diff_count = [diff_count[:self.last_ctrl_patient], diff_count[self.last_ctrl_patient:]]\n",
        "        self.count_val = np.array([0])\n",
        "        self.count_train = np.array([0])\n",
        "        for i in range(len(X)):\n",
        "            nbr_patients =  int(len(patients[i]) *proportion)\n",
        "            start_patient = int(fold_number*nbr_patients )\n",
        "            end_patient = (fold_number+1)*nbr_patients\n",
        "            id_start = patients[i][start_patient]  # segment start\n",
        "            id_end = patients[i][end_patient]  # end segment\n",
        "            if i ==0 :\n",
        "                self.X_val = X[i][id_start:id_end,:,:]\n",
        "                self.X_train = np.delete(X[i], np.arange(id_start,id_end) , 0)\n",
        "\n",
        "                self.y_val = y[i][id_start:id_end]\n",
        "                self.y_train = np.delete(y[i], np.arange(id_start,id_end) , 0)\n",
        "\n",
        "\n",
        "                self.count_val = np.append(self.count_val, diff_count[i][start_patient: end_patient])\n",
        "                self.count_train = np.append(self.count_train, np.delete(diff_count[i], np.arange(start_patient, end_patient)))\n",
        "\n",
        "\n",
        "\n",
        "            else:\n",
        "                start_patient = start_patient #+ patients[0].shape[0]  # patients0.shape 0 is the number of patients in the first class\n",
        "                end_patient =  end_patient# +patients[0].shape[0]\n",
        "                self.X_val = np.vstack((self.X_val, X[i][id_start:id_end,:,:]))\n",
        "                self.X_train = np.vstack((self.X_train, np.delete(X[i], np.arange(id_start,id_end) , 0) ))\n",
        "\n",
        "                self.y_val = np.vstack((self.y_val, y[i][id_start:id_end] ))\n",
        "                self.y_train = np.vstack((self.y_train, np.delete(y[i], np.arange(id_start,id_end) , 0) ))\n",
        "\n",
        "                self.count_val = np.append( self.count_val , diff_count[i][start_patient: end_patient])\n",
        "                self.count_train = np.append(self.count_train,np.delete(diff_count[i], np.arange(start_patient, end_patient)) )\n",
        "\n",
        "        self.count_val = np.cumsum(self.count_val)\n",
        "        self.count_train = np.cumsum(self.count_train )\n",
        "\n",
        "\n",
        "    def load(self, norm = 'std'):\n",
        "        print(\"load training control \")\n",
        "        self.load_data(self.ctrl_list, 0)\n",
        "        if self.deep == 1:\n",
        "            self.last_ctrl= self.X_data.shape[2]\n",
        "            self.last_ctrl_patient = len(self.nb_data_per_person)\n",
        "        print(\"load training parkinson \")\n",
        "\n",
        "\n",
        "        self.load_data(self.pk_list, 1)  # ncycle, nfeature, nombre de data\n",
        "\n",
        "\n",
        "        ## all datas are loaded at this point, preprocessing now\n",
        "        if self.deep == 1:\n",
        "            self.X_data = self.X_data.transpose(2,0 , 1)  #0, 1\n",
        "\n",
        "            if norm == 'std ':\n",
        "                self.normalize()\n",
        "            elif norm == 'l2':\n",
        "                self.X_data = self.normalize_l2(self.X_data)\n",
        "\n",
        "        if self.pk_level:\n",
        "            self.one_hot_encoding()\n",
        "\n",
        "        if self.deep == 1:\n",
        "            self.X_ctrl = self.X_data[:self.last_ctrl]\n",
        "            self.y_ctrl =  self.y_data[:self.last_ctrl]\n",
        "            self.X_park = self.X_data[self.last_ctrl:]\n",
        "            self.y_park = self.y_data[self.last_ctrl:]\n",
        "\n",
        "        print(\"saving training \")\n",
        "        np.save(\"Xdata\", self.X_data)\n",
        "        np.save(\"ydata\", self.y_data)\n",
        "        np.save('data_person',self.nb_data_per_person)\n",
        "        np.save('ctrl_list', self.ctrl_list)\n",
        "        np.save('pk_list', self.pk_list)\n",
        "\n",
        "    def normalize(self):\n",
        "        '''\n",
        "\n",
        "        :return: Normalize to have a mean =  and std =1\n",
        "        '''\n",
        "        mean_train = np.mean(self.X_train,(0,1))\n",
        "        std_train = np.std(self.X_train,(0,1))\n",
        "        self.X_train= (self.X_data - mean_train) / std_train\n",
        "        self.X_test= (self.X_test - mean_train) / std_train\n",
        "    def one_hot_encoding(self):\n",
        "        '''\n",
        "        :return: return one hot encoding vector for severity prediction\n",
        "        '''\n",
        "        self.y_data[self.y_data==0]=0\n",
        "        self.y_data[self.y_data==2]=1\n",
        "        self.y_data[self.y_data==2.5]=2\n",
        "        self.y_data[self.y_data==3]=3\n",
        "        np.set_printoptions(threshold=sys.maxsize)\n",
        "        self.y_data = np.nan_to_num(self.y_data) #Replace NaN with zero\n",
        "        self.y_data = to_categorical(self.y_data)\n",
        "\n",
        "    def normalize_l2(self, data):\n",
        "        '''\n",
        "\n",
        "        :param data:  Function to perform L2 normalization\n",
        "        :return:\n",
        "        '''\n",
        "        data = keras.backend.l2_normalize(data, axis=(1, 2))\n",
        "        data = tf.keras.backend.get_value(data)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def load_data(self, liste, y):\n",
        "        '''\n",
        "\n",
        "        :param liste: list of patients filepaths\n",
        "        :param y: 0 for control, 1 for parkinson\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        for i in range(0, len(liste)):\n",
        "            datas = np.loadtxt(liste[i])  # num cycle, n features\n",
        "            datas = datas[:, self.features_to_load]\n",
        "            print(datas.shape[0])\n",
        "            if  self.pk_level :\n",
        "                y =self.find_level(liste[i])\n",
        "\n",
        "            if self.deep == 1:\n",
        "                X_data, y_data , self.nb_data_per_person = self.generate_datas(datas, y, self.nb_data_per_person)\n",
        "            else:\n",
        "                X_data, y_data = self.generate_datas_ml(datas, y)\n",
        "            if (self.X_data).size == 0:\n",
        "                self.X_data = X_data\n",
        "                self.y_data = y_data\n",
        "            else:\n",
        "                if self.deep == 1:\n",
        "                    self.X_data = np.dstack((self.X_data, X_data))\n",
        "                else:\n",
        "                    self.X_data = np.vstack((self.X_data, X_data))  # shape nb data --- vector size\n",
        "                self.y_data = np.vstack((self.y_data, y_data))\n",
        "\n",
        "            print(X_data.shape, self.X_data.shape)\n",
        "\n",
        "\n",
        "    def find_level(self, file):\n",
        "        start = '../data/'\n",
        "        end = '_'\n",
        "        id = (file.split(start))[1].split(end)[0]\n",
        "        y = self.levels.loc[id, 'HoehnYahr']\n",
        "        return y\n",
        "\n",
        "    def generate_datas(self, datas, y, data_list):\n",
        "        '''\n",
        "\n",
        "        :param datas:  datas loaded for 1 patient\n",
        "        :param y: label of the patient\n",
        "        :param data_list: list containing the number of segments per patients\n",
        "        :return:\n",
        "        '''\n",
        "        count = 0\n",
        "        X_data = np.array([])\n",
        "        y_data = np.array([])\n",
        "        nb_datas = int(datas.shape[0] - self.nb_gait_cycle)\n",
        "        for start in range(0, nb_datas, self.step):\n",
        "            end = start + self.nb_gait_cycle\n",
        "            data = datas[start:end, :]\n",
        "            if X_data.size == 0:\n",
        "                X_data = data\n",
        "                y_data = y\n",
        "            else:\n",
        "                if (self.deep == 1):\n",
        "                    X_data = np.dstack((X_data, data))\n",
        "                else:\n",
        "                    X_data = np.vstack((X_data, data))\n",
        "                y_data = np.vstack((y_data, y))\n",
        "            count = count + 1\n",
        "        data_list = np.append(data_list, count+ data_list[-1])\n",
        "        return X_data, y_data, data_list\n",
        "\n",
        "\n",
        "    def get_datas(self):\n",
        "        return self.X_data, self.y_data, self.X_test, self.y_test, self.X_val, self.y_val\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix,  classification_report, accuracy_score\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "class Results:\n",
        "    def __init__(self, filename_seg, filename_patient):\n",
        "        '''\n",
        "\n",
        "        :param filename_seg:  Filename  (.csv) where to save results at the segment levels\n",
        "        :param filename_patient: Filename  (.csv) where to save results at the patient levels\n",
        "        '''\n",
        "        self.results_patients = np.zeros(3)\n",
        "        self.results_segments = np.zeros(3)\n",
        "        self.filename_seg = filename_seg\n",
        "        self.filename_patient = filename_patient\n",
        "    def add_result( self,res, accuracy,  segments = True  ):\n",
        "        '''\n",
        "\n",
        "        :param res: result of classification report (sklearn )\n",
        "        :param accuracy:\n",
        "        :param segments: 1 to add results at the segment level\n",
        "        :return:\n",
        "        '''\n",
        "        if segments:\n",
        "            specificity = res['0.0']['recall']\n",
        "            sensitivy =  res['1.0']['recall']\n",
        "        else:\n",
        "            specificity = res['0']['recall']\n",
        "            sensitivy =  res['1']['recall']\n",
        "        all = np.array([specificity, sensitivy, accuracy])\n",
        "\n",
        "        if segments:\n",
        "            self.results_segments = np.vstack((self.results_segments, all))\n",
        "        else:\n",
        "            self.results_patients = np.vstack((self.results_patients, all ))\n",
        "\n",
        "    def validate_patient(self, model, x_val, y_val, count):\n",
        "        '''\n",
        "\n",
        "        :param model: trained model after 1 fold of cross validation\n",
        "        :param x_val: x_Val for 1 forld of cross validation\n",
        "        :param y_val: y_Val for 1 forld of cross validation\n",
        "        :param count: vector containing the number of segments per patient\n",
        "        :return:  save the results of the fold\n",
        "\n",
        "        '''\n",
        "        ## per segments\n",
        "        pred_seg = model.predict(np.split(x_val, x_val.shape[2], axis=2))\n",
        "        res = classification_report(np.rint(y_val), np.rint(pred_seg), output_dict = True )\n",
        "        acc = accuracy_score(np.rint(y_val), np.rint(pred_seg))\n",
        "        self.add_result(res, acc,True  )\n",
        "\n",
        "        eval = []\n",
        "        y = []\n",
        "        pred = []\n",
        "        for m in range(1, len(count)):\n",
        "            i = count[m]\n",
        "            j = count[m - 1]\n",
        "            score = model.evaluate(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2), y_val[j:i])\n",
        "            eval.append(score)\n",
        "            y.append(int(np.mean(y_val[j:i])))\n",
        "            p = np.rint(model.predict(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2)))\n",
        "            pred.append(np.mean(p))\n",
        "\n",
        "        res = classification_report(y, np.rint(pred), output_dict = True )\n",
        "        print(classification_report(y, np.rint(pred)))\n",
        "\n",
        "        acc = accuracy_score(np.rint(y), np.rint(pred))\n",
        "        self.add_result(res, acc, False )\n",
        "\n",
        "        #np.savetxt(self.filename_patient, self.results_patients, delimiter=\",\")\n",
        "        #np.savetxt(self.filename_seg, self.results_segments, delimiter=\",\")\n",
        "        res_segments_dict = {'Specificity': self.results_segments[1:,0],'Sensitivity': self.results_segments[1:,1],'Accuracy': self.results_segments[1:,2]  }\n",
        "        df = pd.DataFrame.from_dict(res_segments_dict)\n",
        "        df.to_csv(self.filename_seg)\n",
        "        res_patients_dict =  {'Specificity': self.results_patients[1:,0],'Sensitivity': self.results_patients[1:,1],'Accuracy': self.results_patients[1:,2]  }\n",
        "        df = pd.DataFrame.from_dict(res_patients_dict)\n",
        "        df.to_csv(self.filename_patient)\n",
        "\n",
        "\n",
        "\n",
        "class Results_level:\n",
        "    '''\n",
        "    Class to save results for severity prediction\n",
        "    '''\n",
        "    def __init__(self, filename_seg, filename_patient, dir):\n",
        "        '''\n",
        "        :param filename_seg: filename (csv) where to save the results\n",
        "        :param filename_patient:\n",
        "        :param dir: directory where results files are saved\n",
        "        '''\n",
        "        self.results_patients = np.zeros(1)\n",
        "        self.results_segments = np.zeros(1)\n",
        "        self.filename_seg = filename_seg\n",
        "        self.filename_patient = filename_patient\n",
        "        self.gt = np.array([])\n",
        "        self.pred = np.array([])\n",
        "        self.dir = dir\n",
        "    def add_result( self,res, accuracy,  segments = True  ):\n",
        "\n",
        "        all = np.array([ accuracy])\n",
        "\n",
        "        if segments:\n",
        "            self.results_segments = np.vstack((self.results_segments, all))\n",
        "        else:\n",
        "            self.results_patients = np.vstack((self.results_patients, all ))\n",
        "\n",
        "\n",
        "\n",
        "    def validate_patient(self, model, x_val, y_val, count):\n",
        "        shape=100\n",
        "        '''\n",
        "        :param model: trained model after 1 fold of cross validation\n",
        "        :param x_val: x_Val for 1 forld of cross validation\n",
        "        :param y_val: y_Val for 1 forld of cross validation\n",
        "        :param count: vector containing the number of segments per patient\n",
        "        :return:  save the results of the fold\n",
        "        '''\n",
        "        ## per segments\n",
        "        pred_seg = model.predict(np.split(x_val, x_val.shape[2], axis=2))\n",
        "        y_val_arg = np.argmax(y_val, axis=1)\n",
        "        res = classification_report(np.rint(y_val), np.rint(pred_seg), output_dict = True ) #np.rint(np.argmax(pred_seg, axis=1))\n",
        "        acc = accuracy_score(np.rint(y_val), np.rint(pred_seg)) #np.rint(np.argmax(pred_seg, axis=1))\n",
        "        self.add_result(res, acc,True  )\n",
        "        print('result', res)\n",
        "        print('acc', acc)\n",
        "        eval = []\n",
        "        y = []\n",
        "        pred = []\n",
        "        shape=100\n",
        "        for m in range(1, len(count)):\n",
        "            i = count[m]\n",
        "            j = count[m - 1]\n",
        "            score = model.evaluate(np.split(x_val[j:i, :, :], x_val.shape[2] , axis=2), y_val[j:i])#x_val.shape[2]\n",
        "            eval.append(score)\n",
        "            y_gt = np.argmax(y_val[j:i],1)\n",
        "            y_gt , _ = stats.mode(y_gt, axis = None)\n",
        "            y.append(y_gt[0])\n",
        "            p = np.rint(model.predict(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2))) #x_val.shape[2]\n",
        "            p = np.argmax(p, 1 )\n",
        "            p, _ = stats.mode(p, axis=None)\n",
        "            pred.append(p[0])\n",
        "\n",
        "        res = classification_report(y, np.rint(pred), output_dict = True )\n",
        "        print(classification_report(y, np.rint(pred)))\n",
        "        self.gt = np.append(self.gt, y)\n",
        "        self.pred = np.append(self.pred, np.rint(pred))\n",
        "        acc = accuracy_score(np.rint(y), np.rint(pred))\n",
        "        self.add_result(res, acc, False )\n",
        "        res_segments_dict = {'Accuracy': self.results_segments[1:,0]}\n",
        "        df = pd.DataFrame.from_dict(res_segments_dict)\n",
        "        df.to_csv(self.filename_seg)\n",
        "        res_patients_dict =  {'Accuracy': self.results_patients[1:,0]}\n",
        "        df = pd.DataFrame.from_dict(res_patients_dict)\n",
        "        df.to_csv(self.filename_patient)\n",
        "        print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def write_results(self):\n",
        "        '''\n",
        "        Called at the end to write the final result files\n",
        "        :return:\n",
        "        '''\n",
        "        res_segments_dict = {'Accuracy': self.results_segments[1:,0]}\n",
        "        df = pd.DataFrame.from_dict(res_segments_dict)\n",
        "        df.to_csv(self.filename_seg)\n",
        "        res_patients_dict =  {'Accuracy': self.results_patients[1:,0]}\n",
        "        df = pd.DataFrame.from_dict(res_patients_dict)\n",
        "        df.to_csv(self.filename_patient)\n",
        "        file_pred = os.path.join(self.dir, 'pred.csv')\n",
        "        file_gt = os.path.join(self.dir, 'gt.csv')\n",
        "        np.savetxt(file_pred, self.pred, delimiter=\",\" )\n",
        "        np.savetxt(file_gt,self.gt, delimiter=\",\")\n",
        "        res = classification_report(self.gt, self.pred)\n",
        "        print(res)\n",
        "        self.cm = confusion_matrix(self.gt, self.pred)\n",
        "        file_conf_matrx = os.path.join(self.dir, 'confusion_matrix.csv')\n",
        "        np.savetxt(file_conf_matrx, self.cm, delimiter=\",\")\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(2) #2\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "#from src.data_utils2 import Datas\n",
        "#from src.results import Results,Results_level\n",
        "\n",
        "#from src.algo import multiple_cnn1D, multiple_cnn1D5_level\n",
        "#from src.data_utils import Data\n",
        "\n",
        "def train( model, datas, lr, log_filename, filename):\n",
        "    \"\"\"\n",
        "\n",
        "    :param model: Initial untrained model\n",
        "    :param datas:  data object\n",
        "    :param lr: learning rate\n",
        "    :param log_filename: filename where the training results will be saved ( for each epoch)\n",
        "    :param filename: file where the weights will be saved\n",
        "    :return:  trained model\n",
        "    \"\"\"\n",
        "    X_train = datas.X_train\n",
        "    y_train = datas.y_train\n",
        "    X_val = datas.X_val\n",
        "    y_val = datas.y_val\n",
        "    logger = CSVLogger(log_filename, separator=',', append=True)\n",
        "    for i in (np.arange(1,4)*5):  # 10-20    1-10\n",
        "\n",
        "        #checkpointer = ModelCheckpoint(filepath=filename , monitor='val_acc', verbose=1, save_best_only=True)\n",
        "        checkpointer = ModelCheckpoint(filepath=filename, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=0, mode='auto')\n",
        "\n",
        "        callbacks_list = [checkpointer, early_stopping, logger]\n",
        "\n",
        "        history = model.fit(np.split(X_train,X_train.shape[2], axis=2), \\\n",
        "                            # history  = model.fit(X_data,\\\n",
        "                            y_train, \\\n",
        "                            verbose=1, \\\n",
        "                            shuffle=True, \\\n",
        "                            epochs= 100,\\\n",
        "                            batch_size=400, \\\n",
        "                            validation_data=(np.split(X_val, X_val.shape[2], axis=2), y_val), \\\n",
        "                            callbacks=callbacks_list)\n",
        "\n",
        "        model.load_weights(filename)\n",
        "        lr =  lr / 2\n",
        "        rms = optimizers.Nadam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "        #model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "def train_classifier(args):\n",
        "    '''\n",
        "    Function that performs the detection of Parkinson\n",
        "    :param args: Input arguments\n",
        "    :return:\n",
        "    '''\n",
        "    exp_name = args.exp_name\n",
        "    subfolder = os.path.join(args.output, exp_name +'_' + datetime.datetime.now().strftime(\"%m_%d\"), datetime.datetime.now().strftime(\n",
        "        \"%H_%M\"))\n",
        "    file_result_patients = os.path.join(subfolder,'res_pat.csv')\n",
        "    file_result_segments = os.path.join(subfolder,'res_seg.csv')\n",
        "    model_file = os.path.join(subfolder, \"model.json\")\n",
        "    if not os.path.exists(subfolder):\n",
        "        os.makedirs(subfolder)\n",
        "\n",
        "    val_results = Results(file_result_segments, file_result_patients)\n",
        "    datas = Data(args.input_data, 1, 100, pk_level= False )\n",
        "\n",
        "    for i in range(0, 10):\n",
        "        lr = 0.001\n",
        "        model = multiple_cnn1D(datas.X_data.shape[2])\n",
        "        model_json = model.to_json()\n",
        "        with open(model_file, \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "\n",
        "        print('fold', str(i))\n",
        "        datas.separate_fold(i)\n",
        "        log_filename = os.path.join( subfolder ,\"training_\" + str(i) + \".csv\")\n",
        "        w_filename = os.path.join(subfolder ,\"weights_\" + str(i) + \".keras\")\n",
        "        model = train(model, datas, lr, log_filename, w_filename)\n",
        "        print('Validation !!')\n",
        "        val_results.validate_patient(model, datas.X_val, datas.y_val, datas.count_val)\n",
        "\n",
        "def train_severity(args):\n",
        "    '''\n",
        "\n",
        "    :param args: Input arguments\n",
        "    :return:\n",
        "    '''\n",
        "    features = np.arange(1, 19)\n",
        "\n",
        "\n",
        "    exp_name = args.exp_name\n",
        "\n",
        "    subfolder = os.path.join(args.output, exp_name + '_' + datetime.datetime.now().strftime(\"%m_%d\"), datetime.datetime.now().strftime(\n",
        "        \"%H_%M\"))\n",
        "    if not os.path.exists(subfolder):\n",
        "        os.makedirs(subfolder)\n",
        "    file_result_patients = os.path.join(subfolder ,'res_pat.csv')\n",
        "    file_result_segments = os.path.join(subfolder ,'res_seg.csv')\n",
        "\n",
        "    model_file = os.path.join(subfolder, \"model.json\")\n",
        "\n",
        "    val_results = Results_level(file_result_segments, file_result_patients, subfolder )\n",
        "    datas = Data(args.input_data, 1, 100)  # modif\n",
        "    lr = 0.001\n",
        "    for i in range(0,10):\n",
        "\n",
        "        model = multiple_cnn1D5_level(datas.X_data.shape[2])\n",
        "        model_json = model.to_json()\n",
        "        with open(model_file, \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "        print('fold', str(i))\n",
        "        datas.separate_fold(i)\n",
        "        log_filename = os.path.join(subfolder, \"trainig\" + str(i) + \".csv\")\n",
        "        w_filename = os.path.join(subfolder ,\"weights_\" + str(i) + \".keras\")\n",
        "        model = train(model, datas, lr, log_filename,  w_filename )\n",
        "        print('Validation !!')\n",
        "        val_results.validate_patient(model, datas.X_val, datas.y_val, datas.count_val)\n",
        "        val_results.write_results()\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "def main(args):\n",
        "    if args.exp_name == 'train_classifier' :\n",
        "        train_classifier(args)\n",
        "    elif args.exp_name == 'train_severity':\n",
        "        train_severity(args)\n",
        "    else:\n",
        "        print(\"Unknown experiment name. Please choose from: 'train_classifier', or 'train_severity'.\")\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    input_data='../data',\n",
        "    exp_name='train_severity',\n",
        "    output='train_severity_conv1D_transformer_then_gru_branch_output'\n",
        ")\n",
        "main(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (HCT_env)",
      "language": "python",
      "name": "hct_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
